# RAG con Ollama

## 1. Instalaci√≥n de Ollama

Primero, descarga e instala el launcher de Ollama desde el siguiente enlace:

[Descargar Ollama](https://ollama.com/)

---

## 2. Instalaci√≥n de modelos

Para obtener los modelos necesarios, ejecuta los siguientes comandos:

```bash
ollama pull nomic-embed-text:latest
ollama pull deepseek-r1:1.5b
```

### üîç Verificar que los modelos est√°n correctamente instalados

```bash
ollama list
```

Esto deber√≠a mostrar una lista con los modelos descargados.

---

## 3. Probar el chat directamente

Para verificar que el modelo funciona correctamente, puedes iniciar una conversaci√≥n r√°pida:

```bash
ollama run deepseek-r1:1.5b
```

---

## 4. Configuraci√≥n del entorno virtual

Para mantener las dependencias organizadas, es recomendable crear un entorno virtual:

```bash
python -m venv venv
```

Activa el entorno:

- **Windows:**
  ```bash
  ./venv/Scripts/activate
  ```
- **Linux/Mac:**
  ```bash
  source venv/bin/activate
  ```

---

## 5. Instalar dependencias del proyecto

Una vez activado el entorno virtual, instala las librer√≠as necesarias desde el archivo `requirements.txt`:

```bash
pip install -r requirements.txt
```

---

‚úÖ ¬°Listo! Ahora puedes empezar a trabajar con tu RAG usando Ollama.

---
